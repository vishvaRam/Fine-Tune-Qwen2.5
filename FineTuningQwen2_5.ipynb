{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "kYxHdxyOvaw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVPKSr8WrQ59"
      },
      "outputs": [],
      "source": [
        "# !pip install evaluate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade transformers torch accelerate (Run only if nessary)"
      ],
      "metadata": {
        "id": "4PUaM__5xg1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U datasets fsspec"
      ],
      "metadata": {
        "id": "faapzZj7s2XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import matplotlib.pyplot as plt\n",
        "from evaluate import load\n",
        "import os"
      ],
      "metadata": {
        "id": "jX2vZRoJrU4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "t0lm3OGRDyRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for CUDA availability\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"CUDA not available, using CPU. This will be very slow.\")\n",
        "    device_map = \"cpu\"\n",
        "else:\n",
        "    device_map = \"auto\""
      ],
      "metadata": {
        "id": "fBdF80qPrWSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Before : {tokenizer.pad_token}\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token if it's missing\n",
        "tokenizer.padding_side = \"left\"  # IMPORTANT: Set padding_side to 'left' BEFORE tokenizing\n",
        "print(f\"After : {tokenizer.pad_token}\")\n",
        "\n",
        "# Before : <|endoftext|>\n",
        "# After : <|endoftext|>\n"
      ],
      "metadata": {
        "id": "D_XcJB-ErXrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the dataset\n",
        "dataset_name = \"Vishva007/Databricks-Dolly-4k\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "metadata": {
        "id": "Ecms1XsOrZTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Format the dataset first\n",
        "def format_dolly(sample):\n",
        "    instruction = sample[\"instruction\"]\n",
        "    context = sample[\"context\"]\n",
        "    response = sample[\"response\"]\n",
        "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Context:\\n{context}\\n\\n### Response:\\n{response}\"\n",
        "    return {\"text\": prompt}"
      ],
      "metadata": {
        "id": "jiGAr0RotEud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply formatting\n",
        "dataset = dataset.map(format_dolly).filter(lambda x: x is not None and x[\"text\"] is not None)\n"
      ],
      "metadata": {
        "id": "i-_3QV74tEsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Now tokenize the formatted data\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the texts with padding and truncation\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n"
      ],
      "metadata": {
        "id": "TFgrLH7wtEMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization to create input_ids, attention_mask, etc.\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"instruction\", \"context\", \"response\", \"category\", \"text\"],\n",
        ")"
      ],
      "metadata": {
        "id": "MKAodS9yryoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the tokenized dataset\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]"
      ],
      "metadata": {
        "id": "mcNlWmXmtd8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Configure QLoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        # \"o_proj\",\n",
        "        # \"gate_proj\",\n",
        "        # \"up_proj\",\n",
        "        # \"down_proj\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "q81qzrD8tgaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load the base model in 4-bit quantization with BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sXhPUYR8uM4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        "    # attn_implementation=\"flash_attention_2\" #FlashAttention only supports Ampere GPUs or newer.\n",
        ")"
      ],
      "metadata": {
        "id": "v_woudTwvArb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "EyDtm2syvAcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add LoRA adapters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# trainable params: 1,474,560 || all params: 495,507,328 || trainable%: 0.2976\n"
      ],
      "metadata": {
        "id": "iORLc2WluPCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Set up training arguments\n",
        "output_dir = \"./qwen2_5_dolly_qlora\"  # Directory to save fine-tuned model"
      ],
      "metadata": {
        "id": "E4ao8nnLvHcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "YQj6WvDg1v1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load perplexity metric\n",
        "# perplexity = load(\"perplexity\", module_type=\"metric\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     logits, labels = eval_pred\n",
        "#     predictions = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
        "#     return perplexity.compute(predictions=predictions, references=labels)\n",
        "def compute_metrics(eval_pred):\n",
        "    return {} # Trainer will automatically log eval_loss for us.\n",
        "\n"
      ],
      "metadata": {
        "id": "5NTOS7XnvJCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Set up data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
      ],
      "metadata": {
        "id": "ikkKJVbZvsHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,  # Keep at 1\n",
        "    gradient_accumulation_steps=16,  # Increased to maintain batch size\n",
        "    learning_rate=1e-4,  # Slightly reduced learning rate\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    num_train_epochs=2,  # Reduced from 3 to 2\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,  # Increased eval steps to reduce frequency\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    load_best_model_at_end=False,  # Disabled to save memory\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=20,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataloader_pin_memory=False,  # Disable pin memory to save GPU memory\n",
        "    dataloader_num_workers=0,     # Use single worker to save memory\n",
        "    max_grad_norm=1.0,           # Add gradient clipping\n",
        "    group_by_length=False,       # Disable to save memory\n",
        "    length_column_name=None,\n",
        "    eval_accumulation_steps=1,   # Process eval in smaller chunks\n",
        ")"
      ],
      "metadata": {
        "id": "9LkrVHh9vpfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n"
      ],
      "metadata": {
        "id": "5N02emZ0wC4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Start training\n",
        "# torch.backends.cuda.enable_flash_sdp(True)\n",
        "# torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "# torch.backends.cuda.enable_math_sdp(False)\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
        "# `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
        "#  [226/226 26:53, Epoch 2/2]\n",
        "# Step\tTraining Loss\tValidation Loss\n",
        "# 50\t2.437600\t2.402970\n",
        "# 100\t2.312700\t2.363461\n",
        "# 150\t2.295800\t2.277348\n",
        "# 200\t2.216500\t2.233537\n",
        "\n"
      ],
      "metadata": {
        "id": "kK6AuF8uwHeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Save the fine-tuned LoRA adapters\n",
        "model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "rxDUygZGwS4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store training and evaluation metrics\n",
        "train_history = train_result.metrics\n",
        "eval_history = trainer.evaluate()"
      ],
      "metadata": {
        "id": "9V-33M7Q2nd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_eval_loss = eval_history.get(\"eval_loss\")\n",
        "\n",
        "if final_eval_loss is not None:\n",
        "    final_perplexity = torch.exp(torch.tensor(final_eval_loss)).item()\n",
        "    print(f\"Final Evaluation Loss: {final_eval_loss:.4f}\")\n",
        "    print(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
        "\n",
        "# Final Evaluation Loss: 2.2326\n",
        "# Final Perplexity: 9.32\n"
      ],
      "metadata": {
        "id": "R0TmrjEsru2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 11. Extracting Metrics from log_history for Plotting ---\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "eval_steps = [] # Store steps where evaluation occurred\n",
        "\n",
        "# Iterate through the trainer's log_history\n",
        "for log in trainer.state.log_history:\n",
        "    # Training loss is logged at 'logging_steps' intervals\n",
        "    if \"loss\" in log and \"learning_rate\" in log: # Check if it's a training step log\n",
        "        train_losses.append({\"step\": log[\"step\"], \"loss\": log[\"loss\"]})\n",
        "    # Evaluation loss is logged at 'eval_steps' intervals\n",
        "    if \"eval_loss\" in log:\n",
        "        eval_losses.append({\"step\": log[\"step\"], \"loss\": log[\"eval_loss\"]})\n",
        "\n",
        "# Prepare data for plotting\n",
        "train_steps_plot = [entry[\"step\"] for entry in train_losses]\n",
        "train_values_plot = [entry[\"loss\"] for entry in train_losses]\n",
        "\n",
        "eval_steps_plot = [entry[\"step\"] for entry in eval_losses]\n",
        "eval_values_plot = [entry[\"loss\"] for entry in eval_losses]\n",
        "\n",
        "# --- 12. Plotting the Training and Evaluation Loss ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_steps_plot, train_values_plot, label=\"Training Loss\", marker='.')\n",
        "plt.plot(eval_steps_plot, eval_values_plot, label=\"Evaluation Loss\", marker='o', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Evaluation Loss Over Steps\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plot_path = os.path.join(output_dir, \"loss_plot.png\")\n",
        "plt.savefig(plot_path)\n",
        "print(f\"Loss plot saved to {plot_path}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qj-ubhNC2opj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRkbm90c2tHz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}